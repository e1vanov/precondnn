\documentclass[a4paper, 12pt]{article}
\input{preambule}

\begin{document}

\title{Нейросетевой подход к поиску\\ циркулянтных предобуславливателей \\для систем с теплицевыми матрицами}
\date{Осень 2024}

\maketitle

\tableofcontents

\section{Постановка задачи}

Имеется СЛАУ с теплицевой (пока симметричной матрицей):
\[
    \textbf T\textbf x = \textbf f,
\]
Требуется найти \textit{удачный} правый циркулянтный 
предобуславливатель $\textbf C^{-1}$.
Под удачным понимается предобуславливатель, ускоряющий сходимость
итерационных методов, например, GMRES.

\section{Подбор функционала}

Для начала мы хотим определить функционал / функционалы, которые будем использоваться для обучения сети.
Архитектуру пока отодвигаем в сторону, будем учить многослойный персептрон, который
гарантированно <<выучит все>>. 

Существует несколько общих мотиваций, из которых можно строить функционалы:

\begin{itemize}
    \item На скорость сходимости в итерационных методах
        влияет спектр матрицы $\textbf I - \textbf T\textbf C^{-1}$
        \begin{itemize}
            \item Зажимаем его в ноль равномерно,
                оптимизуем нормы: спектральный радиус,
                вторая норма ($\infty$-норма вектора
                сингулярных чисел), норма Фробениуса (2-норма вектора 
                сингулярных чисел).
            \item На самом деле мы не против небольшого количества
                выборосов среди собственных значений, то есть
                нас интересует (устраивает) малость спектра матрицы
                $\textbf I - \textbf T\textbf C^{-1} - \textbf R$, где
                $\textbf R$ -- матрица малого ранга. 
                В этом случае подходит ядерная норма (1-норма вектора
                сингулярных чисел), но она дорогая для
                вычисления. Звучит, как будто ее можно использовать
                как регуляризатор, но не на постоянной основе, а каждый
                батч / матрицу с вероятностью $p$ штрафовать за нее.
            \item Кроме того, пробуем оптимизироваться на 
                следующий функционал: K-число обусловленности, которое
                определяется как
                \[
                    \frac{(\mathrm{tr} A/n)^n}{\det A}
                \]
                Этот функционал возникает в теоретических выкладках и
                идейно штрафует собственные значений за общий разброс.
                Кроме того, например, если $A=\varepsilon I$, то
                \[
                    \frac{(\mathrm{tr} A/n)^n}{\det A}=
                    \frac{(n\cdot \varepsilon / n)^n}{\varepsilon^n}=1
                \]
                То есть решение задачи оптимизации с таким функционалом
                не единственно и более того значение функционала
                совпадает на матрицах, имеющих
                очень различные свойства с точки зрения сходимости GMRES.
                Как вариант -- использовать его с регуляризатором.
                Другим важным моментом является то, что
                этот функционал следует вычислять не напрямую, а через
                логарифмы, так как что в числителе, что в 
                знаменателе стоят произведения многих чисел.
        \end{itemize}
    \item Можно учиться на быструю разрешимость конкретным
        методом, например, GMRES'ом. В этом подходе видятся 
        несколько проблем:
        \begin{itemize}
            \item Что считать функцией потерь?
                Невязку на $n$-ом шаге? Число итераций до сходимости?
                Штрафовать за число итераций или за распределение
                числа итераций?
            \item Кажется, что учить <<с нуля>> такую сеть
                будет сложно, поскольку понятие <<хороший спектр>>
                для сходимости GMRES немного размыто, мы скорее
                можем указать конкретные случаи, когда
                мы знаем, что сходимость должна быть быстрой, но
                есть ощущение (возможно, с подтверждением из линала), 
                что в начале обучения сеть будет
                много <<путаться>> и в итоге медленно учиться,
                возможно, можно выделить несколько <<голов>> сети
                и надеяться, что каждая выучит свою зависимость.
        \end{itemize}
\end{itemize}

\section{Преобразование Фурье?}

Мы знаем, что существует связь между
спектром теплицевой матрицы $\textbf T$ и частичной суммой ряда Фурье.
Отсюда кажется, что вход нейронной сети есть 
гармоники. Отсюда
возникает интересная связь:
если учить персептрон на гармониках, это
будет <<соответствовать>> обучению сверточной сети во временной области, так
как
\[
    \mathcal F^{-1}(\hat{\textbf{W}}\hat{\textbf{x}})
    =
    \mathcal F^{-1}(\hat{\textbf{W}}) * \mathcal F^{-1}(\hat{\textbf{x}})
    =
    \textbf W * \textbf x
    =
    \int \textbf W(t-\tau)\textbf x(\tau) d\tau
\]
То есть на уровне махания руками учить:
\[
    \textbf{T}\to \mathrm{Perceptron}(\boldsymbol\Theta) \to \textbf{C}^{-1}
\]
Равносильно тому, что учить
\[
    \textbf{T}\to\mathcal F^{-1}\to\mathrm{CNN}(\boldsymbol \Theta) 
    \to\mathcal F\to \textbf{C}^{-1}
\]
Равносильно с точки зрения выразительной способности,
но 
\begin{itemize}
    \item сверточные сети можно учить и применять на данных
        разных размерностей
    \item сверточные сети могут дать тот же скор при меньшем числе параметров
\end{itemize}

\end{document}
