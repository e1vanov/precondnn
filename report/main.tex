\documentclass[a4paper, 12pt]{article}
\input{preambule}

\begin{document}

\title{Нейросетевой подход к поиску\\ циркулянтных предобуславливателей \\для систем с теплицевыми матрицами}
\date{Осень 2024}

\maketitle

\tableofcontents

\section{Постановка задачи}

Имеется СЛАУ с теплицевой (пока симметричной матрицей):
\[
    \textbf T\textbf x = \textbf f,
\]
Требуется найти \textit{удачный} правый циркулянтный 
предобуславливатель $\textbf C^{-1}$.
Под удачным понимается предобуславливатель, ускоряющий сходимость
итерационных методов, например, GMRES.

\section{Подбор функционала}

Для начала мы хотим определить функционал / функционалы, которые будем использоваться для обучения сети.
Архитектуру пока отодвигаем в сторону, будем учить многослойный персептрон, который
гарантированно <<выучит все>>. 

Существует несколько общих мотиваций, из которых можно строить функционалы:

\begin{itemize}
    \item На скорость сходимости в итерационных методах
        влияет спектр матрицы $\textbf I - \textbf T\textbf C^{-1}$
        \begin{itemize}
            \item Зажимаем его в ноль равномерно,
                оптимизуем нормы: спектральный радиус,
                вторая норма ($\infty$-норма вектора
                сингулярных чисел), норма Фробениуса (2-норма вектора 
                сингулярных чисел).
            \item На самом деле мы не против небольшого количества
                выборосов среди собственных значений, то есть
                нас интересует (устраивает) малость спектра матрицы
                $\textbf I - \textbf T\textbf C^{-1} - \textbf R$, где
                $\textbf R$ -- матрица малого ранга. 
                В этом случае подходит ядерная норма (1-норма вектора
                сингулярных чисел), но она дорогая для
                вычисления. Звучит, как будто ее можно использовать
                как регуляризатор, но не на постоянной основе, а каждый
                батч / матрицу с вероятностью $p$ штрафовать за нее.
        \end{itemize}
    \item Можно учиться на быструю разрешимость конкретным
        методом, например, GMRES'ом. В этом подходе видятся 
        несколько проблем:
        \begin{itemize}
            \item Что считать функцией потерь?
                Невязку на $n$-ом шаге? Число итераций до сходимости?
                Штрафовать за число итераций или за распределение
                числа итераций?
            \item Кажется, что учить <<с нуля>> такую сеть
                будет сложно, поскольку понятие <<хороший спектр>>
                для сходимости GMRES немного размыто, мы скорее
                можем указать конкретные случаи, когда
                мы знаем, что сходимость должна быть быстрой, но
                есть ощущение (возможно, с подтверждением из линала), 
                что в начале обучения сеть будет
                много <<путаться>> и в итоге медленно учиться,
                возможно, можно выделить несколько <<голов>> сети
                и надеяться, что каждая выучит свою зависимость.
        \end{itemize}
\end{itemize}

\end{document}
